from pyspark.sql import SparkSession
from pyspark.sql.functions import col, row_number, regexp_replace
from pyspark.sql.window import Window
from pyspark.sql.types import StructType, StructField, StringType
import pandas as pd
from rapidfuzz import fuzz
from tqdm import tqdm
import time

# 1. Spark ì„¸ì…˜ ì‹œì‘
spark = SparkSession.builder \
    .appName("Artist Matching with RapidFuzz") \
    .config("spark.executor.memory", "2g") \
    .getOrCreate()

# 2. ë°ì´í„° ë¡œë“œ
df = spark.read.parquet("hdfs:///user/hadoop/gold/artworks.parquet")

# âœ… 2-1. original_artistì—ì„œ íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±, ê´„í˜¸ë§Œ ìœ ì§€)
df = df.withColumn(
    "original_artist",
    regexp_replace(col("original_artist"), r"[^a-zA-Z0-9ê°€-í£\s()]", "")
)

# 3. original_artist ê³ ìœ ê°’ ìˆ˜ì§‘
artist_list = df.select("original_artist").distinct().dropna().rdd.map(lambda r: r[0]).collect()
artist_list = sorted(set(artist_list))  # ì•ˆì •ì  ìˆœì„œ
print(f"ğŸ¨ ì´ ì‘ê°€ ìˆ˜: {len(artist_list)}")

# 4. Fuzzy matchingìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§
start_time = time.time()  # ì‹œì‘ ì‹œê°„
clusters = []
threshold = 90

for idx, artist in enumerate(tqdm(artist_list, desc="ğŸ” RapidFuzz matching ì¤‘")):
    matched = False
    for cluster in clusters:
        if fuzz.ratio(artist.lower(), cluster[0].lower()) >= threshold:
            cluster.append(artist)
            matched = True
            break
    if not matched:
        clusters.append([artist])

    if (idx + 1) % 100 == 0:
        elapsed = time.time() - start_time
        print(f"[{idx + 1}/{len(artist_list)}] ì™„ë£Œ - ê²½ê³¼ {elapsed:.1f}s, í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(clusters)}")

end_time = time.time()  # ì¢…ë£Œ ì‹œê°„
total_time = end_time - start_time

print(f"âœ… ë§¤ì¹­ ì™„ë£Œ - ì´ {len(clusters)}ê°œ í´ëŸ¬ìŠ¤í„°")
print(f"â±ï¸ ì „ì²´ í´ëŸ¬ìŠ¤í„°ë§ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")

# 5. ëŒ€í‘œ ì‘ê°€ ì¶”ì¶œ + artist_id ë¶€ì—¬ìš© mapping ìƒì„±
cluster_representatives = [group[0] for group in clusters]

# ëª¨ë“  ì›ë³¸ artist â†’ ëŒ€í‘œ artistë¡œ ë§¤í•‘
artist_mapping = {}
for rep in cluster_representatives:
    for group in clusters:
        if group[0] == rep:
            for a in group:
                artist_mapping[a] = rep

# 6. artists í…Œì´ë¸” ìƒì„±
artists_df = pd.DataFrame({
    "original_artist": cluster_representatives,
    "en_artist": [None] * len(cluster_representatives),
    "kor_artist": [None] * len(cluster_representatives)
})

schema = StructType([
    StructField("original_artist", StringType(), True),
    StructField("en_artist", StringType(), True),
    StructField("kor_artist", StringType(), True),
])

spark_artists = spark.createDataFrame(artists_df, schema=schema) \
    .dropDuplicates(["original_artist"]) \
    .withColumn("artist_id", row_number().over(Window.orderBy("original_artist"))) \
    .select("artist_id", "original_artist", "kor_artist", "en_artist")

# ì €ì¥
spark_artists.write.mode("overwrite").parquet("hdfs:///user/hadoop/gold/artists.parquet")

# 7. artworksì— artist_id ì¡°ì¸
mapping_df = pd.DataFrame(artist_mapping.items(), columns=["original", "mapped"])
mapping_spark = spark.createDataFrame(mapping_df)

df = df.join(mapping_spark, df.original_artist == mapping_spark.original, "left") \
       .drop("original") \
       .withColumnRenamed("mapped", "normalized_artist")

df = df.join(spark_artists.select("artist_id", "original_artist"), df.normalized_artist == spark_artists.original_artist, "left") \
       .drop("normalized_artist", "original_artist") \
       .withColumnRenamed("artist_id", "artist_id")

ordered_columns = [
    "artwork_id",
    "museum_id",
    "era",
    "artist_id",
    "original_title",
    "image_url",
    "description",
    "country_origin",
    "created_year",
    "materials",
    "color"
]

df = df.select(*ordered_columns)

# 8. ì €ì¥
df.write.mode("overwrite").parquet("hdfs:///user/hadoop/gold/artworks_with_artist_id.parquet")
