from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.models.variable import Variable
from datetime import datetime, timedelta
from pendulum import timezone
import asyncio
import aiohttp
import json
import time
import logging

local_tz = timezone("Asia/Seoul")

def fetch_harvard_artworks():
    """
    Harvard Art Museums APIë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ë¹„ë™ê¸°ë¡œ ìˆ˜ì§‘í•˜ê³ ,
    /tmp/harvard_artworks.json íŒŒì¼ì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜.
    """
    BASE_URL = "https://api.harvardartmuseums.org/object"
    API_KEY = Variable.get("harvard_api_key")  # Harvard API í‚¤
    LIMIT = 100  # í•œ ë²ˆ ìš”ì²­ ì‹œ ê°€ì ¸ì˜¬ ë°ì´í„° ê°œìˆ˜
    CONCURRENT_REQUESTS = 5  # ë™ì‹œì— ì‹¤í–‰í•  ìš”ì²­ ìˆ˜

    logging.info(f"[DEBUG] Harvard API Key = {API_KEY}")

    PARAMS = {
        "apikey": API_KEY,
        "size": LIMIT
    }

    all_data = []

    async def fetch_page(session, page, max_retries=3):
        """íŠ¹ì • í˜ì´ì§€ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë¹„ë™ê¸° í•¨ìˆ˜ (ìµœëŒ€ max_retriesíšŒ ì¬ì‹œë„)"""
        for attempt in range(max_retries):
            try:
                start_time = time.time()
                params = PARAMS.copy()
                params["page"] = page

                async with session.get(BASE_URL, params=params, timeout=10) as response:
                    response.raise_for_status()
                    data = await response.json()
                    records = data.get("records", [])
                    elapsed_time = time.time() - start_time

                    if not records:
                        logging.info(f"ğŸš¨ [Page {page}] ë°ì´í„° ì—†ìŒ (â± {elapsed_time:.2f}ì´ˆ) â†’ ìˆ˜ì§‘ ì¢…ë£Œ.")
                        return None

                    logging.info(f"âœ… [Page {page}] {len(records)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ (â± {elapsed_time:.2f}ì´ˆ)")
                    return records
            except Exception as e:
                logging.error(f"ğŸš¨ [Page {page}] ìš”ì²­ ì‹¤íŒ¨ (ì‹œë„ {attempt+1}/{max_retries}): {e}")
                await asyncio.sleep(2 ** attempt)
        return []  # ì¬ì‹œë„ í›„ì—ë„ ì‹¤íŒ¨í•˜ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

    async def fetch_all_pages():
        nonlocal all_data
        start_time = time.time()
        page = 1

        async with aiohttp.ClientSession() as session:
            while True:
                logging.info(f"\nğŸš€ ìš”ì²­ ì¤‘: {page} ~ {page + CONCURRENT_REQUESTS - 1} í˜ì´ì§€...")
                tasks = [fetch_page(session, p) for p in range(page, page + CONCURRENT_REQUESTS)]
                results = await asyncio.gather(*tasks)

                for records in results:
                    if records is None:  # ë°ì´í„° ì—†ìŒ â†’ ì¢…ë£Œ
                        logging.info("âœ… ëª¨ë“  ë°ì´í„°ë¥¼ ìˆ˜ì§‘ ì™„ë£Œ!")
                        break
                    all_data.extend(records)

                logging.info(f"ğŸ“Š í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ì´ ë°ì´í„° ê°œìˆ˜: {len(all_data)}")
                page += CONCURRENT_REQUESTS

                if results[-1] is None:
                    break

        elapsed = time.time() - start_time
        logging.info(f"\nğŸ¨ ì´ {len(all_data)}ê°œì˜ ì‘í’ˆ ë°ì´í„°ë¥¼ ìˆ˜ì§‘ ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ)")

        with open("/tmp/harvard_artworks.json", "w", encoding="utf-8") as f:
            json.dump(all_data, f, ensure_ascii=False, indent=4)

    asyncio.run(fetch_all_pages())

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    dag_id='fetch_harvard_artworks',
    default_args=default_args,
    description='Harvard Art Museums API ë°ì´í„° ìˆ˜ì§‘ í›„ HDFS Bronze ë ˆì´ì–´ì— ì €ì¥',
    schedule_interval='@daily',
    start_date=datetime(2025, 3, 31, tzinfo=local_tz),
    catchup=False,
)

# 1. Harvard API ë°ì´í„° ìˆ˜ì§‘ (PythonOperator)
fetch_harvard_task = PythonOperator(
    task_id='fetch_harvard_artworks',
    python_callable=fetch_harvard_artworks,
    dag=dag,
)

# HDFS ê´€ë ¨ ì„¤ì •
hdfs_dir = '/user/hadoop/bronze/harvard'
hdfs_file = f'{hdfs_dir}/Artworks.json'

common_env = {
    "HADOOP_USER_NAME": "root",
    "JAVA_HOME": "/opt/java",
    "PATH": "/opt/hadoop/bin:/opt/java/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
}


mkdir_task = BashOperator(
    task_id='make_hdfs_bronze_harvard_dir',
    env=common_env,
    bash_command=f'hdfs dfs -mkdir -p {hdfs_dir}',
    dag=dag,
)

upload_task = BashOperator(
    task_id='upload_harvard_artworks_to_hdfs',
    env=common_env,
    bash_command=f'hdfs dfs -put -f /tmp/harvard_artworks.json {hdfs_file}',
    dag=dag,
)

fetch_harvard_task >> mkdir_task >> upload_task
